{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3G210dRTvXb",
    "outputId": "8bafe492-fcb7-438b-c657-6a86ea644a6b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 5.5 MB 13.5 MB/s \n",
      "\u001B[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "\u001B[K     |████████████████████████████████| 182 kB 69.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 7.6 MB 57.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ray[tune]\n",
      "  Downloading ray-2.1.0-cp37-cp37m-manylinux2014_x86_64.whl (59.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 59.1 MB 1.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.19.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.1.1)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n",
      "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.50.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (22.1.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.6)\n",
      "Collecting virtualenv>=20.0.24\n",
      "  Downloading virtualenv-20.16.7-py3-none-any.whl (8.8 MB)\n",
      "\u001B[K     |████████████████████████████████| 8.8 MB 56.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.4)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.10)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001B[K     |████████████████████████████████| 125 kB 72.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.32.0->ray[tune]) (1.15.0)\n",
      "Collecting platformdirs<3,>=2.4\n",
      "  Downloading platformdirs-2.5.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.24->ray[tune]) (4.13.0)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001B[K     |████████████████████████████████| 468 kB 68.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.3->virtualenv>=20.0.24->ray[tune]) (3.10.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.19.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2022.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2022.9.24)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n",
      "Installing collected packages: platformdirs, distlib, virtualenv, tensorboardX, ray\n",
      "Successfully installed distlib-0.3.6 platformdirs-2.5.4 ray-2.1.0 tensorboardX-2.5.1 virtualenv-20.16.7\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install transformers\n",
    "!pip install \"ray[tune]\"\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyMEWxv4TxM5"
   },
   "outputs": [],
   "source": [
    "no_train_epochs = 4\n",
    "freeze_layer_count = 4\n",
    "pretrained_model_tokenizer_path = r\"distilroberta-base\"\n",
    "df_input = pd.read_csv(r\"\")\n",
    "df_input_val = pd.read_csv(r\"\")\n",
    "df_test = pd.read_csv(r\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mVFFFnIAT2Mi",
    "outputId": "89fb5e7d-4ea6-480b-e4b6-86c872800d68"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 590\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 444\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='444' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [444/444 00:32, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>0.392029</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.797155</td>\n",
       "      <td>0.795597</td>\n",
       "      <td>0.775986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.284311</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.885351</td>\n",
       "      <td>0.885351</td>\n",
       "      <td>0.885351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.376759</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.901730</td>\n",
       "      <td>0.901730</td>\n",
       "      <td>0.901730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.400775</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.903683</td>\n",
       "      <td>0.899240</td>\n",
       "      <td>0.901212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.421840</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.903683</td>\n",
       "      <td>0.899240</td>\n",
       "      <td>0.901212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>0.468290</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.894362</td>\n",
       "      <td>0.892296</td>\n",
       "      <td>0.893268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[48 24]\n",
      " [ 4 49]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[65  7]\n",
      " [ 7 46]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[66  6]\n",
      " [ 6 47]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[67  5]\n",
      " [ 7 46]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[67  5]\n",
      " [ 7 46]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[66  6]\n",
      " [ 7 46]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs\n",
      "Configuration saved in /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs/pytorch_model.bin\n",
      "loading configuration file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing done\n",
      "[1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 0 1]\n",
      "Confusion Matrix:\n",
      "[[75  5]\n",
      " [11 34]]\n",
      "Test scores\n",
      "Accuracy: 0.872\n",
      "F1: 0.8565691336775674\n",
      "Precision: 0.8719439475253429\n",
      "Recall: 0.8465277777777778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_tokenizer_path)\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    print(confusion_matrix(labels, pred))\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_tokenizer_path, num_labels=2)\n",
    "\n",
    "for layer in model.roberta.encoder.layer[:freeze_layer_count]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "df_input = df_input.sample(frac=1)\n",
    "\n",
    "text_train = list(df_input['text'])\n",
    "label_train = list(df_input['label'])\n",
    "\n",
    "df_input_val = df_input_val.sample(frac=1)\n",
    "\n",
    "text_val = list(df_input_val['text'])\n",
    "label_val = list(df_input_val['label'])\n",
    "\n",
    "#text_train, text_val, label_train, label_val = train_test_split(text, label, test_size=0.2)\n",
    "text_train_tokenized = tokenizer(text_train, padding=True, truncation=True, max_length=100)\n",
    "text_val_tokenized = tokenizer(text_val, padding=True, truncation=True, max_length=100)\n",
    "\n",
    "train_dataset = Dataset(text_train_tokenized, label_train)\n",
    "val_dataset = Dataset(text_val_tokenized, label_val)\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_tokenizer_path, return_dict=True)\n",
    "    \n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=no_train_epochs,\n",
    "    seed=200,\n",
    ")\n",
    "\n",
    "#args = TrainingArguments(\"test\", save_strategy=\"epoch\", save_total_limit=1, evaluation_strategy=\"steps\", eval_steps=500, disable_tqdm=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    #model_init=model_init,\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "'''best_trial = trainer.hyperparameter_search(\n",
    "            backend=\"ray\",\n",
    "            direction='maximize',\n",
    "            n_trials=10,\n",
    "       )'''\n",
    "\n",
    "#RUNNING    | 172.28.0.2:9174 |     1.12076e-05 |                  4 |                     16 |  1.89943 \n",
    "trainer.train()\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Create torch dataset\n",
    "df_test = df_test.sample(frac=1)\n",
    "\n",
    "text_test = list(df_test['text'])\n",
    "label_test = list(df_test['label'])\n",
    "\n",
    "text_test_tokenized = tokenizer(text_test, padding=True, truncation=True, max_length=100)\n",
    "test_dataset = Dataset(text_test_tokenized)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "\n",
    "print(\"Testing done\")\n",
    "print(y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(label_test, y_pred))\n",
    "\n",
    "test_f1 = f1_score(y_true=label_test, y_pred=y_pred, average='macro')\n",
    "test_accuracy = accuracy_score(y_true=label_test, y_pred=y_pred)\n",
    "test_recall = recall_score(y_true=label_test, y_pred=y_pred, average='macro')\n",
    "test_precision = precision_score(y_true=label_test, y_pred=y_pred, average='macro')\n",
    "\n",
    "print(\"Test scores\")\n",
    "print(\"Accuracy: {}\\nF1: {}\\nPrecision: {}\\nRecall: {}\\n\".format(test_accuracy, test_f1, test_precision, test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2XF5uw7jk5ye",
    "outputId": "9a37149b-3ba1-4c23-98e0-e7c7d0a1f65c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file /content/drive/MyDrive/Thesis/model_dr_df_gayrights_het_20e/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/model_dr_df_gayrights_het_20e\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /content/drive/MyDrive/Thesis/model_dr_df_gayrights_het_20e/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/model_dr_df_gayrights_het_20e\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file /content/drive/MyDrive/Thesis/model_dr_df_gayrights_het_20e/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/model_dr_df_gayrights_het_20e\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file /content/drive/MyDrive/Thesis/finetuned_dr_df_gayRights_het_contrastive_20_4_epochs/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/finetuned_dr_df_gayRights_het_contrastive_20_4_epochs\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /content/drive/MyDrive/Thesis/finetuned_dr_df_gayRights_het_contrastive_20_4_epochs/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Thesis/finetuned_dr_df_gayRights_het_contrastive_20_4_epochs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 207\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.5767654  -0.7248151 ]\n",
      " [-0.66268396  0.6675228 ]\n",
      " [-0.01091881 -0.03802142]\n",
      " [ 1.313789   -1.466486  ]\n",
      " [-0.593473    0.6783388 ]\n",
      " [-1.435955    1.5116082 ]\n",
      " [-0.7051036   0.679502  ]\n",
      " [ 0.33301216 -0.24528293]\n",
      " [-0.50969166  0.6779821 ]\n",
      " [-0.688125    0.84002566]\n",
      " [ 0.8571471  -1.0969044 ]\n",
      " [ 0.32382756 -0.43421003]\n",
      " [-0.3754681   0.3760809 ]\n",
      " [-1.6378511   1.7588665 ]\n",
      " [ 0.527103   -0.5236211 ]\n",
      " [-0.8370453   0.9310567 ]\n",
      " [ 1.2860839  -1.582164  ]\n",
      " [-2.3191764   2.445303  ]\n",
      " [-1.8710716   1.9750698 ]\n",
      " [ 1.1299139  -1.3155646 ]\n",
      " [-0.3562359   0.33243665]\n",
      " [ 0.6632202  -0.71466273]\n",
      " [-2.069438    2.1197567 ]\n",
      " [-2.1473763   2.2932649 ]\n",
      " [-1.779853    1.9149762 ]\n",
      " [-1.8105823   1.823574  ]\n",
      " [-1.5573834   1.5718364 ]\n",
      " [-0.3731844   0.41250715]\n",
      " [ 1.064401   -1.4343277 ]\n",
      " [-2.012571    2.1962478 ]\n",
      " [ 0.6146445  -0.764507  ]\n",
      " [-2.6920815   2.9445598 ]\n",
      " [-0.5509793   0.4730823 ]\n",
      " [-1.4928645   1.4528519 ]\n",
      " [-1.1093079   1.1370183 ]\n",
      " [-2.0570986   2.1974428 ]\n",
      " [ 1.626082   -1.8582761 ]\n",
      " [ 0.2815325  -0.24298345]\n",
      " [-0.49202496  0.4250187 ]\n",
      " [ 0.28615472 -0.28460267]\n",
      " [ 1.0974694  -1.1447558 ]\n",
      " [-1.2324117   1.362814  ]\n",
      " [ 2.0177853  -2.3606286 ]\n",
      " [ 2.2299733  -2.3788278 ]\n",
      " [-1.794128    1.9420093 ]\n",
      " [-1.1579871   1.3234831 ]\n",
      " [-1.9084872   1.9549894 ]\n",
      " [-1.2863843   1.4417369 ]\n",
      " [-1.0328215   1.1202717 ]\n",
      " [ 1.5454551  -1.6110511 ]\n",
      " [-2.2124135   2.4506018 ]\n",
      " [-0.49342108  0.54102415]\n",
      " [ 0.49230614 -0.6319299 ]\n",
      " [-1.234243    1.280552  ]\n",
      " [-2.7072096   2.821151  ]\n",
      " [-1.8488687   1.8252347 ]\n",
      " [-0.17880213  0.28760892]\n",
      " [-2.5089505   2.5990167 ]\n",
      " [-2.2896352   2.5446582 ]\n",
      " [ 2.100189   -2.3411598 ]\n",
      " [-2.0086563   2.2111313 ]\n",
      " [ 2.2110243  -2.3426218 ]\n",
      " [-1.5370666   1.6878661 ]\n",
      " [ 1.5931644  -1.7980807 ]\n",
      " [-1.8446127   1.9613612 ]\n",
      " [-1.4699131   1.718007  ]\n",
      " [ 0.9339503  -0.87281907]\n",
      " [ 0.94164246 -1.1411881 ]\n",
      " [ 1.127137   -1.3107693 ]\n",
      " [-1.8392228   1.9990706 ]\n",
      " [ 0.43203562 -0.66102314]\n",
      " [-1.4242011   1.6089734 ]\n",
      " [-0.4177938   0.41058972]\n",
      " [-0.46476087  0.57468975]\n",
      " [-1.8309048   2.0678113 ]\n",
      " [-0.03539588  0.08293741]\n",
      " [ 0.02109406 -0.10083107]\n",
      " [-0.33233082  0.29481688]\n",
      " [-1.6159108   1.6995064 ]\n",
      " [-1.6920496   1.7676266 ]\n",
      " [ 0.23002581 -0.21467698]\n",
      " [ 1.554551   -1.8299412 ]\n",
      " [-2.3838515   2.5069788 ]\n",
      " [-1.6116923   1.8395336 ]\n",
      " [-0.16873181  0.3815057 ]\n",
      " [ 0.44624016 -0.43828103]\n",
      " [-0.61899745  0.91666234]\n",
      " [-1.9262778   2.0884538 ]\n",
      " [-0.32342547  0.35722914]\n",
      " [ 1.9699823  -2.2804136 ]\n",
      " [ 2.046183   -2.2914    ]\n",
      " [ 1.1339303  -1.1392274 ]\n",
      " [-1.7244579   1.849071  ]\n",
      " [-1.3772224   1.4478291 ]\n",
      " [-1.990565    2.0940335 ]\n",
      " [-0.5428914   0.4259006 ]\n",
      " [-1.6251645   1.6117458 ]\n",
      " [-1.298108    1.2010436 ]\n",
      " [-1.344518    1.2331275 ]\n",
      " [-0.5868638   0.63281715]\n",
      " [ 1.7553911  -1.948311  ]\n",
      " [ 1.1629503  -1.2752918 ]\n",
      " [ 0.46832496 -0.5677651 ]\n",
      " [ 1.8419418  -2.031397  ]\n",
      " [ 1.0250863  -1.130811  ]\n",
      " [ 1.1472101  -1.4051104 ]\n",
      " [-0.6662583   0.6914968 ]\n",
      " [ 1.2357308  -1.3544093 ]\n",
      " [-1.2312634   1.4262242 ]\n",
      " [-1.9033114   1.9205097 ]\n",
      " [-0.13031827  0.09147731]\n",
      " [-1.3427022   1.4257238 ]\n",
      " [ 0.15457167 -0.26372555]\n",
      " [ 0.17573266 -0.17607637]\n",
      " [-0.5709436   0.52297896]\n",
      " [-1.4874771   1.587732  ]\n",
      " [-2.0879726   2.1950142 ]\n",
      " [-2.1304643   2.2624772 ]\n",
      " [-1.4834385   1.8035135 ]\n",
      " [-0.72031194  0.7231152 ]\n",
      " [-1.1087347   1.3820533 ]\n",
      " [-0.39047524  0.36461172]\n",
      " [-0.07819656  0.00546761]\n",
      " [ 1.813465   -1.9901291 ]\n",
      " [-0.5975402   0.5185973 ]\n",
      " [ 1.2491336  -1.3404895 ]\n",
      " [-2.1266122   2.4246776 ]\n",
      " [-1.2274995   1.3961467 ]\n",
      " [-2.2468135   2.4578683 ]\n",
      " [ 0.33742076 -0.5326536 ]\n",
      " [-0.70350313  0.7748237 ]\n",
      " [-0.57853687  0.630571  ]\n",
      " [-1.2163506   1.1734473 ]\n",
      " [ 0.31101662 -0.49659997]\n",
      " [-1.280621    1.2744898 ]\n",
      " [ 1.712055   -2.042453  ]\n",
      " [ 1.96024    -2.0434785 ]\n",
      " [ 1.3426398  -1.3256772 ]\n",
      " [ 0.58807546 -0.6310164 ]\n",
      " [-0.9180626   0.9859881 ]\n",
      " [-2.3097672   2.4264135 ]\n",
      " [ 0.25634134 -0.24408945]\n",
      " [-0.56391233  0.55092096]\n",
      " [-2.5577235   2.7773328 ]\n",
      " [-0.32725137  0.3427006 ]\n",
      " [-0.62238854  0.532398  ]\n",
      " [-0.15300661  0.158291  ]\n",
      " [ 0.4133808  -0.50979394]\n",
      " [-0.05826251 -0.05445728]\n",
      " [-0.89970416  1.1181377 ]\n",
      " [ 0.10830659 -0.11081466]\n",
      " [-1.9180216   2.0181692 ]\n",
      " [-1.2172624   1.2607704 ]\n",
      " [-0.2139853   0.31648016]\n",
      " [-0.37961528  0.27511734]\n",
      " [-2.3421469   2.5110087 ]\n",
      " [ 1.4528879  -1.7748386 ]\n",
      " [ 2.6886797  -3.1266356 ]\n",
      " [ 1.0755503  -1.0971929 ]\n",
      " [-0.71860087  0.74923754]\n",
      " [ 0.9050963  -1.1900811 ]\n",
      " [-0.5869893   0.825316  ]\n",
      " [ 1.520438   -1.6533664 ]\n",
      " [-0.25302356  0.4669356 ]\n",
      " [-2.028911    2.0999935 ]\n",
      " [-2.573129    2.7681923 ]\n",
      " [ 1.9238633  -2.2345464 ]\n",
      " [-2.3201613   2.476298  ]\n",
      " [ 0.25438747 -0.3149512 ]\n",
      " [ 2.299285   -2.602829  ]\n",
      " [-1.25965     1.1158893 ]\n",
      " [-0.37093985  0.2765923 ]\n",
      " [-2.6941016   2.8159246 ]\n",
      " [-0.94828045  0.9814124 ]\n",
      " [ 2.294909   -2.5180717 ]\n",
      " [-1.5561149   1.429403  ]\n",
      " [-2.1901429   2.3317306 ]\n",
      " [-0.06061981  0.02565176]\n",
      " [-1.7177199   1.8464617 ]\n",
      " [ 1.0508969  -1.136832  ]\n",
      " [-1.5335947   1.5730873 ]\n",
      " [ 0.63521415 -1.0339144 ]\n",
      " [-1.4273288   1.5018345 ]\n",
      " [-0.05394479 -0.07046308]\n",
      " [-2.5324612   2.6361556 ]\n",
      " [ 1.4070305  -1.3823327 ]\n",
      " [-0.909048    1.0168833 ]\n",
      " [ 1.5747528  -1.7772416 ]\n",
      " [ 1.8590032  -2.1680577 ]\n",
      " [-1.4058858   1.3790292 ]\n",
      " [-0.4471445   0.6287722 ]\n",
      " [-0.8265904   0.98646575]\n",
      " [-1.2004766   1.3438646 ]\n",
      " [-0.8935055   0.9240121 ]\n",
      " [-0.5552192   0.5196691 ]\n",
      " [ 2.974079   -3.2484949 ]\n",
      " [-1.5466621   1.7610897 ]\n",
      " [-0.4411631   0.45035002]\n",
      " [-2.0785928   2.2690678 ]\n",
      " [-0.71418315  0.8178903 ]\n",
      " [-0.9237211   0.9161298 ]\n",
      " [-2.2689471   2.4188666 ]\n",
      " [-0.8294649   1.041698  ]\n",
      " [-0.13031751  0.08162928]\n",
      " [ 0.67302114 -0.9506505 ]\n",
      " [-1.2137947   1.3827115 ]\n",
      " [-1.5586364   1.7411548 ]]\n",
      "Testing done\n",
      "[0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0\n",
      " 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
      " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1\n",
      " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1]\n",
      "Confusion Matrix:\n",
      "[[32 43]\n",
      " [39 93]]\n",
      "Test scores For Negation\n",
      "Accuracy: 0.6038647342995169\n",
      "F1: 0.5661930075649152\n",
      "Precision: 0.5672638773819387\n",
      "Recall: 0.5656060606060607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_tokenizer_path = r\"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_tokenizer_path)\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "model_path = \"\"\n",
    "\n",
    "#_perturbed_negation_baseline_100\n",
    "df_test_adv_neg = pd.read_csv(r\"\")\n",
    "\n",
    "text_test_adv_neg = list(df_test_adv_neg['text'])\n",
    "label_test_adv_neg = list(df_test_adv_neg['label'])\n",
    "\n",
    "text_test_adv_neg_tokenized = tokenizer(text_test_adv_neg, padding=True, truncation=True, max_length=100)\n",
    "test_dataset_adv_neg = Dataset(text_test_adv_neg_tokenized)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_adv_neg_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred_adv_neg, _, _ = test_adv_neg_trainer.predict(test_dataset_adv_neg)\n",
    "\n",
    "raw_pred_adv_neg = raw_pred_adv_neg[0]\n",
    "# Preprocess raw predictions\n",
    "y_pred_adv_neg = np.argmax(raw_pred_adv_neg, axis=1)\n",
    "\n",
    "print(raw_pred_adv_neg)\n",
    "print(\"Testing done\")\n",
    "print(y_pred_adv_neg)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(label_test_adv_neg, y_pred_adv_neg))\n",
    "\n",
    "\n",
    "test_adv_neg_f1 = f1_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg, average='macro')\n",
    "test_adv_neg_accuracy = accuracy_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg)\n",
    "test_adv_neg_recall = recall_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg, average='macro')\n",
    "test_adv_neg_precision = precision_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg, average='macro')\n",
    "\n",
    "print(\"Test scores For Negation\")\n",
    "print(\"Accuracy: {}\\nF1: {}\\nPrecision: {}\\nRecall: {}\\n\".format(test_adv_neg_accuracy, test_adv_neg_f1, test_adv_neg_precision, test_adv_neg_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_LKGIhb4k6_U",
    "outputId": "d0bca9ce-8575-46cc-917e-993d0c5b8492",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_baseline_20_t6_epochs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing done\n",
      "[0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
      " 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 1 1]\n",
      "Confusion Matrix:\n",
      "[[76  4]\n",
      " [11 34]]\n",
      "Test scores for Spelling\n",
      "Accuracy: 0.88\n",
      "F1: 0.8647283745761489\n",
      "Precision: 0.8841500302480338\n",
      "Recall: 0.8527777777777777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"\"\n",
    "\n",
    "df_test_adv_neg = pd.read_csv(r\"\")\n",
    "\n",
    "text_test_adv_spell = list(df_test_adv_neg['text'])\n",
    "label_test_adv_spell = list(df_test_adv_neg['label'])\n",
    "\n",
    "text_test_adv_spell_tokenized = tokenizer(text_test_adv_spell, padding=True, truncation=True, max_length=100)\n",
    "test_dataset_adv_spell = Dataset(text_test_adv_spell_tokenized)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_adv_spell_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred_adv_spell, _, _ = test_adv_spell_trainer.predict(test_dataset_adv_spell)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred_adv_spell = np.argmax(raw_pred_adv_spell, axis=1)\n",
    "\n",
    "print(\"Testing done\")\n",
    "print(y_pred_adv_spell)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(label_test_adv_spell, y_pred_adv_spell))\n",
    "\n",
    "test_adv_spell_f1 = f1_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell, average='macro')\n",
    "test_adv_spell_accuracy = accuracy_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell)\n",
    "test_adv_spell_recall = recall_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell, average='macro')\n",
    "test_adv_spell_precision = precision_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell, average='macro')\n",
    "\n",
    "print(\"Test scores for Spelling\")\n",
    "print(\"Accuracy: {}\\nF1: {}\\nPrecision: {}\\nRecall: {}\\n\".format(test_adv_spell_accuracy, test_adv_spell_f1, test_adv_spell_precision, test_adv_spell_recall))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}