{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install transformers\n",
    "!pip install \"ray[tune]\"\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyb9tFKn3lIC",
    "outputId": "b0ff8b3d-451a-49cd-cbd0-f531520ac8e8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 5.5 MB 32.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 7.6 MB 52.7 MB/s \n",
      "\u001B[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001B[K     |████████████████████████████████| 163 kB 60.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ray[tune]\n",
      "  Downloading ray-2.1.0-cp37-cp37m-manylinux2014_x86_64.whl (59.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 59.1 MB 1.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.2.0)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.4)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n",
      "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.50.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.6)\n",
      "Collecting virtualenv>=20.0.24\n",
      "  Downloading virtualenv-20.16.6-py3-none-any.whl (8.8 MB)\n",
      "\u001B[K     |████████████████████████████████| 8.8 MB 61.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.8.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (22.1.0)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001B[K     |████████████████████████████████| 125 kB 70.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.10)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.32.0->ray[tune]) (1.15.0)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001B[K     |████████████████████████████████| 468 kB 50.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.24->ray[tune]) (4.13.0)\n",
      "Collecting platformdirs<3,>=2.4\n",
      "  Downloading platformdirs-2.5.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.3->virtualenv>=20.0.24->ray[tune]) (3.10.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.19.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2022.9.24)\n",
      "Installing collected packages: platformdirs, distlib, virtualenv, tensorboardX, ray\n",
      "Successfully installed distlib-0.3.6 platformdirs-2.5.3 ray-2.1.0 tensorboardX-2.5.1 virtualenv-20.16.6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "no_train_epochs = 4\n",
    "freeze_layer_count = 4\n",
    "pretrained_model_tokenizer_path = r\"\"\n",
    "df_input = pd.read_csv(r\"\")\n",
    "df_input_val = pd.read_csv(r\"\")\n",
    "df_test = pd.read_csv(r\"\")\n",
    "model_path = \"\"\n"
   ],
   "metadata": {
    "id": "oD_yuWmW3ros"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-leND7pJ3epc",
    "outputId": "47cf70d0-38a7-43f6-afdc-a6ffcc5472d6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at /content/drive/MyDrive/Thesis/model_dr_ksdt_ht_e20 were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Thesis/model_dr_ksdt_ht_e20 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 590\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 296\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='296' max='296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [296/296 00:24, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.306800</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.889769</td>\n",
       "      <td>0.880372</td>\n",
       "      <td>0.884075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.965199</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.862028</td>\n",
       "      <td>0.861120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.931871</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.862028</td>\n",
       "      <td>0.861120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.989421</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.868182</td>\n",
       "      <td>0.871462</td>\n",
       "      <td>0.869588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[67  5]\n",
      " [ 9 44]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[63  9]\n",
      " [ 8 45]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[63  9]\n",
      " [ 8 45]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[63  9]\n",
      " [ 7 46]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs\n",
      "Configuration saved in /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs/pytorch_model.bin\n",
      "loading configuration file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing done\n",
      "[1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0\n",
      " 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 0 1]\n",
      "Confusion Matrix:\n",
      "[[73  7]\n",
      " [ 9 36]]\n",
      "Test scores\n",
      "Accuracy: 0.872\n",
      "F1: 0.8597081930415265\n",
      "Precision: 0.8637266023823029\n",
      "Recall: 0.85625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_tokenizer_path)\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred[0], axis=1)\n",
    "\n",
    "    print(confusion_matrix(labels, pred))\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_tokenizer_path, num_labels=2)\n",
    "\n",
    "'''for param in model.roberta.parameters():\n",
    "    param.requires_grad = False'''\n",
    "\n",
    "for layer in model.roberta.encoder.layer[:freeze_layer_count]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "df_input = df_input.sample(frac=1)\n",
    "\n",
    "text_train = list(df_input['text'])\n",
    "label_train = list(df_input['label'])\n",
    "\n",
    "df_input_val = df_input_val.sample(frac=1)\n",
    "\n",
    "text_val = list(df_input_val['text'])\n",
    "label_val = list(df_input_val['label'])\n",
    "\n",
    "#text_train, text_val, label_train, label_val = train_test_split(text, label, test_size=0.2)\n",
    "text_train_tokenized = tokenizer(text_train, padding=True, truncation=True, max_length=100)\n",
    "text_val_tokenized = tokenizer(text_val, padding=True, truncation=True, max_length=100)\n",
    "\n",
    "train_dataset = Dataset(text_train_tokenized, label_train)\n",
    "val_dataset = Dataset(text_val_tokenized, label_val)\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_tokenizer_path, return_dict=True)\n",
    "    \n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=no_train_epochs,\n",
    "    seed=200,\n",
    ")\n",
    "\n",
    "#args = TrainingArguments(\"test\", save_strategy=\"epoch\", save_total_limit=1, evaluation_strategy=\"steps\", eval_steps=500, disable_tqdm=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    #model_init=model_init,\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "'''best_trial = trainer.hyperparameter_search(\n",
    "            backend=\"ray\",\n",
    "            direction='maximize',\n",
    "            n_trials=10,\n",
    "       )'''\n",
    "\n",
    "#RUNNING    | 172.28.0.2:9174 |     1.12076e-05 |                  4 |                     16 |  1.89943 \n",
    "trainer.train()\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Create torch dataset\n",
    "df_test = df_test.sample(frac=1)\n",
    "\n",
    "text_test = list(df_test['text'])\n",
    "label_test = list(df_test['label'])\n",
    "\n",
    "text_test_tokenized = tokenizer(text_test, padding=True, truncation=True, max_length=100)\n",
    "test_dataset = Dataset(text_test_tokenized)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred[0], axis=1)\n",
    "\n",
    "print(\"Testing done\")\n",
    "print(y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(label_test, y_pred))\n",
    "\n",
    "test_f1 = f1_score(y_true=label_test, y_pred=y_pred, average='macro')\n",
    "test_accuracy = accuracy_score(y_true=label_test, y_pred=y_pred)\n",
    "test_recall = recall_score(y_true=label_test, y_pred=y_pred, average='macro')\n",
    "test_precision = precision_score(y_true=label_test, y_pred=y_pred, average='macro')\n",
    "\n",
    "print(\"Test scores\")\n",
    "print(\"Accuracy: {}\\nF1: {}\\nPrecision: {}\\nRecall: {}\\n\".format(test_accuracy, test_f1, test_precision, test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pretrained_model_tokenizer_path = r\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_tokenizer_path)\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "model_path = \"\"\n",
    "\n",
    "df_test_adv_neg = pd.read_csv(r\"\")\n",
    "\n",
    "text_test_adv_neg = list(df_test_adv_neg['text'])\n",
    "label_test_adv_neg = list(df_test_adv_neg['label'])\n",
    "\n",
    "text_test_adv_neg_tokenized = tokenizer(text_test_adv_neg, padding=True, truncation=True, max_length=100)\n",
    "test_dataset_adv_neg = Dataset(text_test_adv_neg_tokenized)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_adv_neg_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred_adv_neg, _, _ = test_adv_neg_trainer.predict(test_dataset_adv_neg)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred_adv_neg = np.argmax(raw_pred_adv_neg[0], axis=1)\n",
    "\n",
    "'''print(raw_pred_adv_neg)\n",
    "print(\"Testing done\")\n",
    "print(y_pred_adv_neg)'''\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(label_test_adv_neg, y_pred_adv_neg))\n",
    "\n",
    "test_adv_neg_f1 = f1_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg, average='macro')\n",
    "test_adv_neg_accuracy = accuracy_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg)\n",
    "test_adv_neg_recall = recall_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg, average='macro')\n",
    "test_adv_neg_precision = precision_score(y_true=label_test_adv_neg, y_pred=y_pred_adv_neg, average='macro')\n",
    "\n",
    "print(\"Test scores For Negation\")\n",
    "print(\"Accuracy: {}\\nF1: {}\\nPrecision: {}\\nRecall: {}\\n\".format(test_adv_neg_accuracy, test_adv_neg_f1, test_adv_neg_precision, test_adv_neg_recall))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W4kzlAfKaLN6",
    "outputId": "e2081799-b357-48aa-cf72-2a7dcd6b7491"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_het_contrastive_20_4_epochs/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/finetuned_dr_ksdt_het_contrastive_20_4_epochs\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_het_contrastive_20_4_epochs/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_het_contrastive_20_4_epochs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix:\n",
      "[[70 10]\n",
      " [ 9 36]]\n",
      "Test scores For Negation\n",
      "Accuracy: 0.848\n",
      "F1: 0.8358559679314397\n",
      "Precision: 0.8343423225096313\n",
      "Recall: 0.8375\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_path = \"\"\n",
    "\n",
    "df_test_adv_spell = pd.read_csv(r\"\")\n",
    "\n",
    "text_test_adv_spell = list(df_test_adv_spell['text'])\n",
    "label_test_adv_spell = list(df_test_adv_spell['label'])\n",
    "\n",
    "text_test_adv_spell_tokenized = tokenizer(text_test_adv_spell, padding=True, truncation=True, max_length=100)\n",
    "test_dataset_adv_spell = Dataset(text_test_adv_spell_tokenized)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_adv_spell_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred_adv_spell, _, _ = test_adv_spell_trainer.predict(test_dataset_adv_spell)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred_adv_spell = np.argmax(raw_pred_adv_spell[0], axis=1)\n",
    "\n",
    "print(\"Testing done\")\n",
    "print(y_pred_adv_spell)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(label_test_adv_spell, y_pred_adv_spell))\n",
    "\n",
    "test_adv_spell_f1 = f1_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell, average='macro')\n",
    "test_adv_spell_accuracy = accuracy_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell)\n",
    "test_adv_spell_recall = recall_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell, average='macro')\n",
    "test_adv_spell_precision = precision_score(y_true=label_test_adv_spell, y_pred=y_pred_adv_spell, average='macro')\n",
    "\n",
    "print(\"Test scores for Spelling\")\n",
    "print(\"Accuracy: {}\\nF1: {}\\nPrecision: {}\\nRecall: {}\\n\".format(test_adv_spell_accuracy, test_adv_spell_f1, test_adv_spell_precision, test_adv_spell_recall))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eZMtMLIncpx1",
    "outputId": "e117c4c4-e93b-416e-ef18-e3ee7409356b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Thesis/finetuned_dr_ksdt_ht_contrastive_20_4_epochs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 125\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing done\n",
      "[0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1\n",
      " 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1\n",
      " 0 0 1 1 1 1 0 0 0 1 0 0 1 1]\n",
      "Confusion Matrix:\n",
      "[[70 10]\n",
      " [ 8 37]]\n",
      "Test scores for Spelling\n",
      "Accuracy: 0.856\n",
      "F1: 0.8452118877270226\n",
      "Precision: 0.8423349699945445\n",
      "Recall: 0.8486111111111111\n",
      "\n"
     ]
    }
   ]
  }
 ]
}